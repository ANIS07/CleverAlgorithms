<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
  <head>
    <meta http-equiv="Content-Type" content="application/xhtml+xml; charset=utf-8" />
    <title>Clever Algorithms</title>
    <link rel="stylesheet" href="main.css" type="text/css" />
  </head>
  <body>
    
<a id='particle_swarm_optimization'><h1>Particle Swarm Optimization</h1></a>
<p>
<em>Particle Swarm Optimization, PSO.</em>
</p>

<a id='taxonomy'><h2>Taxonomy</h2></a>
<p>
Particle Swarm Optimization belongs to the field of Swarm Intelligence and Collective Intelligence and is a sub-field of Computational Intelligence.
Particle Swarm Optimization is related to other Swarm Intelligence algorithms such as Ant Colony Optimization and it is a baseline algorithm for many variations, too numerous to list.
</p>


<a id='inspiration'><h2>Inspiration</h2></a>
<p>
Particle Swarm Optimization is inspired by the social foraging behavior of some animals such as flocking behavior of birds and the schooling behavior of fish.
</p>


<a id='metaphor'><h2>Metaphor</h2></a>
<p>
Particles in the swarm fly through an environment following the fitter members of the swarm and generally biasing their movement toward historically good areas of their environment.
</p>


<a id='strategy'><h2>Strategy</h2></a>
<p>
The goal of the algorithm is to have all the particles locate the optima in a multi-dimensional hyper-volume.
This is achieved by assigning initially random positions to all particles in the space and small initial random velocities. The algorithm is executed like a simulation, advancing the position of each particle in turn based on its velocity, the best known global position in the problem space and the best position known to a particle. The objective function is sampled after each position update. Over time, through a combination of exploration and exploitation of known good positions in the search space, the particles cluster or converge together around an optima, or several optima.
</p>


<a id='procedure'><h2>Procedure</h2></a>
<p>
The Particle Swarm Optimization algorithm is comprised of a collection of particles that move around the search space influenced by their own best past location and the best past location of the whole swarm or a close neighbor. Each iteration a particle's velocity is updated using:
</p>
$v_{i}(t+1) = v_{i}(t) + \big( c_1 \times rand() \times (p_{i}^{best} - p_{i}(t)) \big) +   \big( c_2 \times rand() \times (p_{gbest} - p_{i}(t)) \big)$
<p>
where $v_{i}(t+1)$ is the new velocity for the $i^{th}$ particle, $c_1$ and $c_2$ are the weighting coefficients for the personal best and global best positions respectively, $p_{i}(t)$ is the $i^{th}$ particle's position at time $t$, $p_{i}^{best}$ is the $i^{th}$ particle's best known position, and $p_{gbest}$ is the best position known to the swarm. The $rand()$ function generate a uniformly random variable $\in [0,1]$. Variants on this update equation consider best positions within a particles local neighborhood at time $t$.
</p>
<p>
A particle's position is updated using:
</p>
$p_{i}(t+1) = p_{i}(t) + v_{i}(t)$
<p>
Algorithm (below) provides a pseudocode listing of the Particle Swarm Optimization algorithm for minimizing a cost function.
</p>
<div class='pseudocode'>
<strong><strong><code>Input</code></strong></strong>: 
<code>ProblemSize</code>, $Population_{size}$
<br />
<strong><strong><code>Output</code></strong></strong>: 
$P_{g\_best}$
<br />
<code>Population</code> $\leftarrow$ $\emptyset$<br />
$P_{g\_best}$ $\leftarrow$ $\emptyset$<br />
<strong><code>For</code></strong> ($i=1$ <strong><code>To</code></strong> $Population_{size}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;$P_{velocity}$ $\leftarrow$ <code>RandomVelocity</code>()<br />
&nbsp;&nbsp;&nbsp;&nbsp;$P_{position}$ $\leftarrow$ <code>RandomPosition</code>($Population_{size}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;$P_{cost}$ $\leftarrow$ <code>Cost</code>($P_{position}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;$P_{p\_best}$ $\leftarrow$ $P_{position}$<br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>If</code></strong> ($P_{cost}$ $\leq$ $P_{g\_best}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$P_{g\_best}$ $\leftarrow$ $P_{p\_best}$<br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>End</code></strong><br />
<strong><code>End</code></strong><br />
<strong><code>While</code></strong> ($\neg$<code>StopCondition</code>())<br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>For</code></strong> ($P$ $\in$ <code>Population</code>)<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$P_{velocity}$ $\leftarrow$ <code>UpdateVelocity</code>($P_{velocity}$, $P_{g\_best}$, $P_{p\_best}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$P_{position}$ $\leftarrow$ <code>UpdatePosition</code>($P_{position}$, $P_{velocity}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$P_{cost}$ $\leftarrow$ <code>Cost</code>($P_{position}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>If</code></strong> ($P_{cost}$ $\leq$ $P_{p\_best}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$P_{p\_best}$ $\leftarrow$ $P_{position}$<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>If</code></strong> ($P_{cost}$ $\leq$ $P_{g\_best}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$P_{g\_best}$ $\leftarrow$ $P_{p\_best}$<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>End</code></strong><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>End</code></strong><br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>End</code></strong><br />
<strong><code>End</code></strong><br />
<strong><code>Return</code></strong> ($P_{g\_best}$)<br />
</div>
<div class='caption'>Pseudocode for PSO.</div>



<a id='heuristics'><h2>Heuristics</h2></a>
<ul>
<li> The number of particles should be low, around 20-40</li>
<li> The speed a particle can move (maximum change in its position per iteration) should be bounded, such as to a percentage of the size of the domain.</li>
<li> The learning factors (biases towards global and personal best positions) should be between 0 and 4, typically 2.</li>
<li> A local bias (local neighborhood) factor can be introduced where neighbors are determined based on Euclidean distance between particle positions.</li>
<li> Particles may leave the boundary of the problem space and may be penalized, be reflected back into the domain or biased to return back toward a position in the problem domain. Alternatively, a wrapping strategy may be used at the edge of the domain creating a loop, torrid or related geometrical structures at the chosen dimensionality.</li>
<li> An inertia or momentum coefficient can be introduced to limit the change in velocity.</li>
</ul>


<a id='code_listing'><h2>Code Listing</h2></a>
<p>
Listing (below) provides an example of the Particle Swarm Optimization algorithm implemented in the Ruby Programming Language.
The demonstration problem is an instance of a continuous function optimization that seeks $\min f(x)$ where $f=\sum_{i=1}^n x_{i}^2$, $-5.0\leq x_i \leq 5.0$ and $n=3$. The optimal solution for this basin function is $(v_0,\ldots,v_{n-1})=0.0$.
The algorithm is a conservative version of Particle Swarm Optimization based on the seminal papers. The implementation limits the velocity at a pre-defined maximum, and bounds particles to the search space, reflecting their movement and velocity if the bounds of the space are exceeded. Particles are influenced by the best position found as well as their own personal best position. Natural extensions may consider limiting velocity with an inertia coefficient and including a neighborhood function for the particles.
</p>
<pre class='prettyprint lang-rb'>
def objective_function(vector)
  return vector.inject(0.0) {|sum, x| sum +  (x ** 2.0)}
end

def random_vector(minmax)
  return Array.new(minmax.size) do |i|
    minmax[i][0] + ((minmax[i][1] - minmax[i][0]) * rand())
  end
end

def create_particle(search_space, vel_space)
  particle = {}
  particle[:position] = random_vector(search_space)
  particle[:cost] = objective_function(particle[:position])
  particle[:b_position] = Array.new(particle[:position])
  particle[:b_cost] = particle[:cost]
  particle[:velocity] = random_vector(vel_space)
  return particle
end

def get_global_best(population, current_best=nil)
  population.sort{|x,y| x[:cost] &lt;=&gt; y[:cost]}
  best = population.first
  if current_best.nil? or best[:cost] &lt;= current_best[:cost]
    current_best = {}
    current_best[:position] = Array.new(best[:position])
    current_best[:cost] = best[:cost]
  end
  return current_best
end

def update_velocity(particle, gbest, max_v, c1, c2)
  particle[:velocity].each_with_index do |v,i|
    v1 = c1 * rand() * (particle[:b_position][i] - particle[:position][i])
    v2 = c2 * rand() * (gbest[:position][i] - particle[:position][i])
    particle[:velocity][i] = v + v1 + v2
    particle[:velocity][i] = max_v if particle[:velocity][i] &gt; max_v
    particle[:velocity][i] = -max_v if particle[:velocity][i] &lt; -max_v
  end
end

def update_position(part, bounds)
  part[:position].each_with_index do |v,i|
    part[:position][i] = v + part[:velocity][i]
    if part[:position][i] &gt; bounds[i][1]
      part[:position][i]=bounds[i][1]-(part[:position][i]-bounds[i][1]).abs
      part[:velocity][i] *= -1.0
    elsif part[:position][i] &lt; bounds[i][0]
      part[:position][i]=bounds[i][0]+(part[:position][i]-bounds[i][0]).abs
      part[:velocity][i] *= -1.0
    end
  end
end

def update_best_position(particle)
  return if particle[:cost] &gt; particle[:b_cost]
  particle[:b_cost] = particle[:cost]
  particle[:b_position] = Array.new(particle[:position])
end

def search(max_gens, search_space, vel_space, pop_size, max_vel, c1, c2)
  pop = Array.new(pop_size) {create_particle(search_space, vel_space)}
  gbest = get_global_best(pop)
  max_gens.times do |gen|
    pop.each do |particle|
      update_velocity(particle, gbest, max_vel, c1, c2)
      update_position(particle, search_space)
      particle[:cost] = objective_function(particle[:position])
      update_best_position(particle)
    end
    gbest = get_global_best(pop, gbest)
    puts " &gt; gen #{gen+1}, fitness=#{gbest[:cost]}"
  end
  return gbest
end

if __FILE__ == $0
  # problem configuration
  problem_size = 2
  search_space = Array.new(problem_size) {|i| [-5, 5]}
  # algorithm configuration
  vel_space = Array.new(problem_size) {|i| [-1, 1]}
  max_gens = 100
  pop_size = 50
  max_vel = 100.0
  c1, c2 = 2.0, 2.0
  # execute the algorithm
  best = search(max_gens, search_space, vel_space, pop_size, max_vel, c1,c2)
  puts "done! Solution: f=#{best[:cost]}, s=#{best[:position].inspect}"
end
</pre>
<div class='download_src'><a href='pso.rb'>Download Source</a></div>
<div class='caption'>Particle Swarm Optimization in Ruby</div>


<a id='references'><h2>References</h2></a>

<a id='primary_sources'><h3>Primary Sources</h3></a>
<p>
Particle Swarm Optimization was described as a stochastic global optimization method for continuous functions in 1995 by Eberhart and Kennedy  [<a href='#Eberhart1995'>Eberhart1995</a>] [<a href='#Kennedy1995'>Kennedy1995</a>]. This work was motivated as an optimization method loosely based on the flocking behavioral models of Reynolds  [<a href='#Reynolds1987'>Reynolds1987</a>].
Early works included the introduction of inertia  [<a href='#Shi1998'>Shi1998</a>] and early study of social topologies in the swarm by Kennedy  [<a href='#Kennedy1999'>Kennedy1999</a>].
</p>


<a id='learn_more'><h3>Learn More</h3></a>
<p>
Poli, Kennedy, and Blackwell provide a modern overview of the field of PSO with detailed coverage of extensions to the baseline technique  [<a href='#Poli2007'>Poli2007</a>]. Poli provides a meta-analysis of PSO publications that focus on the application the technique, providing a systematic breakdown on application areas  [<a href='#Poli2008a'>Poli2008a</a>].
An excellent book on Swarm Intelligence in general with detailed coverage of Particle Swarm Optimization is &quot;Swarm Intelligence&quot; by Kennedy, Eberhart, and Shi  [<a href='#Kennedy2001'>Kennedy2001</a>].
</p>




<h2>Bibliography</h2>
<table>
 <tr valign="top">
 <td><a id='Eberhart1995'>[Eberhart1995]</a></td>
 <td>R. C. Eberhart and J. Kennedy, "<a href="http://scholar.google.com.au/scholar?q=A+new+optimizer+using+particle+swarm+theory">A new optimizer using particle swarm theory</a>", in Proceedings of the sixth international symposium on micro machine 	and human science, 1995.</td>
 </tr>
 <tr valign="top">
 <td><a id='Kennedy1995'>[Kennedy1995]</a></td>
 <td>J. Kennedy and R. C. Eberhart, "<a href="http://scholar.google.com.au/scholar?q=Particle+swarm+optimization">Particle swarm optimization</a>", in Proceedings IEEE int'l conf. on neural networks Vol. IV, 1995.</td>
 </tr>
 <tr valign="top">
 <td><a id='Kennedy1999'>[Kennedy1999]</a></td>
 <td>J. Kennedy, "<a href="http://scholar.google.com.au/scholar?q=Small+Worlds+and+Mega-Minds:+Effects+of+Neighborhood+Topology+on++Particle+Swarm+Performance">Small Worlds and Mega-Minds: Effects of Neighborhood Topology on 	Particle Swarm Performance</a>", in Proceedings of the 1999 Congress on Evolutionary Computation, 1999.</td>
 </tr>
 <tr valign="top">
 <td><a id='Kennedy2001'>[Kennedy2001]</a></td>
 <td>J. Kennedy and R. C. Eberhart and Y. Shi, "<a href="http://scholar.google.com.au/scholar?q=Swarm+Intelligence">Swarm Intelligence</a>", Morgan Kaufmann, 2001.</td>
 </tr>
 <tr valign="top">
 <td><a id='Poli2007'>[Poli2007]</a></td>
 <td>R. Poli and J. Kennedy and T. Blackwell, "<a href="http://scholar.google.com.au/scholar?q=Particle+swarm+optimization+An+overview">Particle swarm optimization An overview</a>", Swarm Intelligence, 2007.</td>
 </tr>
 <tr valign="top">
 <td><a id='Poli2008a'>[Poli2008a]</a></td>
 <td>R. Poli, "<a href="http://scholar.google.com.au/scholar?q=Analysis+of+the+publications+on+the+applications+of+particle+swarm++optimisation">Analysis of the publications on the applications of particle swarm 	optimisation</a>", Journal of Artificial Evolution and Applications, 2008.</td>
 </tr>
 <tr valign="top">
 <td><a id='Reynolds1987'>[Reynolds1987]</a></td>
 <td>C. W. Reynolds, "<a href="http://scholar.google.com.au/scholar?q=Flocks,+herds+and+schools:+A+distributed+behavioral+model">Flocks, herds and schools: A distributed behavioral model</a>", in Proceedings of the 14th annual conference on Computer graphics and 	interactive techniques, 1987.</td>
 </tr>
 <tr valign="top">
 <td><a id='Shi1998'>[Shi1998]</a></td>
 <td>Y. Shi and R. C. Eberhart, "<a href="http://scholar.google.com.au/scholar?q=A+Modified+Particle+Swarm+Optimizers">A Modified Particle Swarm Optimizers</a>", in Proceedings of the IEEE International Conference on Evolutionary 	Computation, 1998.</td>
 </tr>
</table>


  </body>
</html>  
