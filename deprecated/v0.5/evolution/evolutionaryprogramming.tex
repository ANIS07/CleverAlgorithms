\section{Evolutionary Programming}

\subsection{Inspiration}
The inspiration for evolutionary programming is the theory of evolution by means of natural selection.

\subsection{Strategy}
The evolutionary programming strategy is focused on the product of the evolutionary process for problem solving.

The Evolutionary Programming (EP) algorithm is focused on evolution at the species level as a method of artificial intelligence, contrasted with heuristic-driven approaches. The strategy is concerned with the macro properties of evolution (adaptive fit in an environment) over micro concerns such as manipulating and concerning sub-symbolic information in artificial genomes.

\subsection{Procedure}
Traditional realizations of EP make use of a variation operator applied to each member of the population independently (no recombination operator for example) and a competitive selection process between parents and offspring for the limited number of positions in the subsequent generation. The algorithm models problems using domain specific representations.

\begin{lstlisting}
Initialize the population
While not stop condition
	Evaluate
	Select breeding set
	Create offspring
	Select new population from union of old and new populations
End
\end{lstlisting}

More recently the field has sought to merge with the broader field of evolutionary computation, incorporating conventional genetic operators and application to a broader set of problem domains. As such, modern EP bares a strong resemblance to ES given the now standard use of self-adaptive variance operators.

\subsection{Heuristics}

\begin{itemize}
	\item Heuristics to govern the mutation of self-adaptive strategy variables as well as objective variables
\end{itemize}

\subsection{Tutorial}

\subsubsection{Introduction}
This tutorial provides a guide for implementing the Evolutionary Programming algorithm applied to a the non-linear programming problem with continuous parameters called Rastigen's Function.

\subsubsection{Problem}
The problem is a non-linear programming problem that defines a function with a set of unknown parameters. Such problems are common for testing optimization problems because the number of problems can be varied, to vary the difficulty of the problem. 

\begin{lstlisting}
def calculate(v)
  (10.0 * @dimensions.to_f) + v.inject(0) {|sum, x| (x**2.0) - 10.0 * Math.cos(2.0 * Math::PI * x) }
end
\end{lstlisting}

The problem space is a hypercube with the boundaries of -5.12 and +5.12. The solution to the problem is known where each objective parameter has the value `0'.

\subsubsection{Solution}
A solution to the problem is represented by an array of floating point numbers (vector). In Associated with the solution and its fitness scoring is an array of strategy parameters, one for each objective parameter. These strategy parameters control the variance genetic operator applied to each objective parameter. 

The population is seeded with candidate solutions seeded with random objective values from within the search space. During the execution of the search, each candidate solution creates a progeny candidate solution from the parents objective and strategy vectors. The variation operator is introduced per-objective parameter and implemented in the transcription of the vectors.

\begin{lstlisting}
def initialize_offspring(parent, length, min, max)
  # populate strategy parameters
  g = Random::next_gaussian
  @strategy_params = Array.new(length) do |i|
    transcribe_strategy(parent.strategy_params[i], g, length)
  end
  # populate objective values
  @objective_params = Array.new(length) do |i|
    transcribe_objective(parent.objective_params[i], @strategy_params[i], min, max)
  end
end
\end{lstlisting}

The mutation of the strategy occurs based on some heuristics proportional to the number of parameters in the problem.

\begin{lstlisting}
def transcribe_strategy(x, g, dimensions)
  x * Math::exp((heuristic_rprime(dimensions) * g) + (heuristic_r(dimensions) * Random::next_gaussian))
end
\end{lstlisting}

Give that a solution is represented by floating point numbers, a Gaussian-based mutation operator is applied. This operator involves the re-sampling an objective parameter using a Normal distribution where the current value represents the mean, and the standard deviation is determined by the corresponding strategy parameter. New objective values are bounds tested to ensure they are valid in the context of the problem (within the constraints of the problems parameter space).

\begin{lstlisting}
def transcribe_objective(x, stdev, min, max)
  o = x + stdev * Random::next_gaussian
  o = min if o < min
  o = max if o > max
  return o
end
\end{lstlisting}

\subsubsection{Algorithm}
The Evolutionary Programming algorithm is implemented as a reusable system for executing the strategy on a problem and producing an approximation of the optimal solution. The strategy is executed by calling the evolve function that initializes the population and executes each generation until the stop condition is met. In this case, the stop condition is defined as a set number of generations or whether the known optimal solution has been discovered.

\begin{lstlisting}
def evolve(problem)
  # store problem
  @problem = problem
  # prepare the initial population
  initialize_population
  # evaluate the initial population
  evaluate_population(population)
  # evolve until stop condition is triggered
  while !should_stop? do
    # create the new population
    @population = evolve_population(@population)
  end
end
\end{lstlisting}

A single generation involves the creation of a new population of offspring (one per parent), and selecting the candidate solutions from the parent and child populations to comprise the next generation. 

\begin{lstlisting}
def evolve_population(population)
  # recombine and mutate
  offspring = Array.new(population.length) do |i|
    @problem.new_solution_offspring(population[i])
  end
  # evaluate the newly created candidate solutions
  evaluate_population(offspring)
  # combine the existing and new populations
  union = population + offspring
  # let the solutions compete for survival    
  competitive_tournaments(union)
  # shuffle the union in case few solutions win
  Random.shuffle_array(union)
  # order the union by wins desc
  union.sort! { |a,b| b.wins <=> a.wins }
  # select the winners for the new population
  winners = union[0...population.length]
  # one more generation has completed
  @generation += 1
  puts "generation:#{@generation}, #{@best_solution}"    
  return winners
end
\end{lstlisting}

Competition involves combining the parent and offspring populations and for each member of this new set, comparing its fitness to a set number of opponents. If the candidate solutions is better than all opponents which it faces then it is awarded a point. After all bouts are completed the next generation of candidate solutions is selected based on the number of wins each solution was awarded during the competitive process.

\begin{lstlisting}
def competitive_tournaments(pop)
  # clear wins
  pop.each {|s| s.wins = 0 }
  # to get a point, each solution must better than a set of random peers
  pop.each do |s|
    better = true
    heuristic_num_opponents.times do |i|
      pos = Random::next_int(pop.length)
      better = false if @problem.is_better?(s, pop[pos])
    end
    s.wins += 1 if better
  end
end
\end{lstlisting}

\subsection{Summary}
The Evolutionary Programming algorithm is very similar to ES, and in this case was demonstrated without a recombination operator and with a `soft' selection mechanism designed to ensure the search is not too greedy. Natural extensions involves alternative heuristics for adapting the strategy parameters, the use of a recombination operator and varied selection mechanisms.

\subsubsection{Further Reading}

\begin{itemize}
	\item Evolutionary programming made faster (1999)
	\item Book by L Fogel: Artificial Intelligence through Simulated Evolution
	\item EP on Scholarpedia: \url{http://www.scholarpedia.org/article/Evolutionary_programming}
\end{itemize}