\section{Differential Evolution}

\subsection{Inspiration}
Differential Evolution is inspired by the theory of evolution by means of natural selection.

\subsection{Strategy}
The strategy for Differential Evolution involves perturbing existing candidate solutions based on the weighted difference between existing candidate solutions.

The Differential Evolution (DE) algorithm involves maintaining a population of candidate solutions that under go generations of recombination, evaluation, and selection. The recombination strategy is quite different to similar evolutionary algorithms involving the creation of new candidate solution components based on the weighted difference between two randomly selected population members added to a third population member (so-called classic DE). This approach perturbs population members relative relative to the spread of the broader population. In conjunction with selection, the perturbation effect self-organizes or bound the resampling of the problem space to known areas of interest.

\subsection{Procedure}
The Differential Evolution algorithm involves the maintenance of a population of candidate solutions that are exposed to repeated rounds of recombination, evaluation and selection referred to as generations. 

The algorithm as a specialized nomenclature for each variation of the technique describing the specifics of the configuration used in the variation. This takes the form of DE/x/y/z, where x represents the solution to be perturbed such a random or best (in the population). The y signifies the number of difference vectors used in the perturbation of x, where a difference vectors is essentially the difference between two randomly selected although distinct members of the population. Finally, z signifies the recombination operator performed such as bin for binomial and exp for exponential. 

The classical DE is noted as DE/rand/1/bin, where as variation of this approach that creates new solutions as perturbations of the best solution in the generation is noted as DE/best/1/bin.

\subsection{Heuristics}
rules of thumb

\begin{itemize}
	\item The number of parents (NP) should be 10 times the number of parameters being optimized
	\item Use a weighting factor (F) of 0.8
	\item Use a crossover rate (CR) of 0.9
	\item Initialize new vectors anywhere in the valid search space
	\item Increase to NP should decrease F and vice-versa
	\item Higher crossover for DE/rand/1/bin than for DE/rand/1/exp
	\item CR and F almost always in [0.5, 1.0]
	\item Common approachs: DE/rand/1/ DE/best/2/
\end{itemize}

\subsection{Tutorial}

\subsubsection{Introduction}
This tutorial demonstrate how to implement the classical differential evolution algorithm also known as DE/rand/1/bin. The algorithm is applied to a non linear programming problem with continuous parameters called Rosenbrock's Valley.

\subsubsection{Problem}
The problem is a non-linear programming problem called Rosenbrock's Valley that defines an equation with a number of continuous-valued parameters. The scope of valid parameters is between -2.048 and 2.048, and parameters are added linearly and as such can be scaled to increase the difficulty of the problem. The optimal solution is located at 1.0 in each dimension and evaluates to 0.0 in this minimization problem.

\begin{lstlisting}
def calculate(v)
  sum = 0.0
  v.each_with_index do |x, i|
    sum += 100 * (((v[i+1] - (x**2.0)) ** 2.0) + ((1.0 - x) ** 2))  if i < v.length-1
  end
  return sum
end
\end{lstlisting}

\subsubsection{Solution}
A candidate solution to this problem is defined as a vector of floating point values, one for each dimension of the problem. A new candidate solution is initialized to a random position within the constraints of the problem definition.

\begin{lstlisting}
def initialize_random(length, min, max)
  # random objective parameters
  @vector = Array.new(length) {|i| Random.next_float_bounded(min, max)}
end
\end{lstlisting}

During recombination, a new candidate solution is created from and competes with each member of the population p0. The recombination operator takes three randomly selected although distinct population members. During the creation of the offspring, at least one crossover event must occur, where as the average event is defined by a crossover rate. When a crossover event occurs, new values are calculated as the value from one of the randomly selected population members (P3) perturbed by a weighted difference vector between two other population members (P1 and P2). 

\begin{lstlisting}
def initialize_offspring(p0, p1, p2, p3, length, min, max)
  @vector = Array.new(length)
  forced_cross = rand(length)
  @vector.fill do |i|
    if (i==forced_cross or rand < heuristic_crossover_factor)
      transcribe(p3.vector[i] + heuristic_weighting_Factor * (p1.vector[i] - p2.vector[i]), min, max)
    else
      p0.vector[i]
    end
  end
end
\end{lstlisting}

The effect is that points are projected based on the average spread of the population. Selection within the algorithm means that the spread of the population will be reduced over time to areas of interest within the problem space. The weighted difference vector approach therefore self-organizes re-sampling of the problem space, making use of existing information in a way quite different from most other evolutionary search algorithms.

Newly assigned values pass through a transcribe function that ensures that the values are within the constraints of the problem definition.

\begin{lstlisting}
def transcribe(x, min, max)
  return min if x < min
  return max if x > max
  return x
end
\end{lstlisting}

\subsubsection{Algorithm}
The algorithm involves the initialization and evolution of a population of candidate solutions over a number of generations.

\begin{lstlisting}
def evolve(problem)
  # initialize the system
  @problem = problem
  @best_solution = nil
  @generation = 0
  # prepare the initial population
  @population = Array.new(heuristic_population_size) { |i| @problem.new_solution }
  # evaluate the initial population
  evaluate_population(population)
  # evolve until stop condition is triggered
  @population = evolve_population(@population) until should_stop?
end
\end{lstlisting}

A single generation involves creating a new candidate offspring from and to compete with each member of the population. This involves selecting three random although distinct candidate solutions from the population to participate in recombination for each candidate solution in the current population.

\begin{lstlisting}
def evolve_population(population)
  # create offspring
  offspring = Array.new(population.length)
  population.each_with_index do |current, index|
    p1 = p2 = p3 = -1
    p1 = rand(population.length) until p1!=index
    p2 = rand(population.length) until p2!=index and p2!=p1
    p3 = rand(population.length) until p3!=index and p3!=p1 and p3!=p2
    offspring[index] = @problem.new_solution_offspring(current, population[p1], population[p2], population[p3])
  end
  # evaluate
  evaluate_population(offspring)
  # compete for survival
  new_population = Array.new(population.length) {|i| @problem.choose_better(population[i], offspring[i])}
  # one more generation has completed
  @generation += 1
  puts "generation:#{@generation}, #{@best_solution}"    
  return new_population
end
\end{lstlisting}

\subsection{Summary}
The algorithm is deceptively simple, and the weighted difference vector approach to re-sampling the solution space requires some careful consideration. Frankly it amazes me that the approach works at all, let a lone as competitive as it does.

Natural extensions to this implementation are to implement the other DE approaches. There are some great code examples in `new ideas in optimization' as well as a good listing in `New Optimization Techniques in Engineering'.

\subsubsection{Further Reading}

\begin{itemize}
	\item Differential Evolution - In Search of Solutions (2006)
	\item Differential Evolution - A Practical Approach to Global Optimization (2005)
	\item Advances in Differential Evolution ()
	\item New Ideas in Optimization (1999), great section on DE
	\item New Optimization Techniques in Engineering
	\item Sample Code: \url{http://www.icsi.berkeley.edu/~storn/code.html}
	\item Bibliography: \url{http://www2.lut.fi/~jlampine/debiblio.htm}
\end{itemize}