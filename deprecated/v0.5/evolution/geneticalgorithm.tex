\section{Genetic Algorithm}

\subsection{Inspiration}
The inspiration for the Genetic Algorithm is a genetic perspective of the theory of evolution. The theory of evolution by means of natural selection accounts for the diversity of life of the planet. It describes a situation where organisms reproduce in quantities greater than can be supported by the environment, the uniqueness of individuals in the population given low frequency genetic mutations during cell division, and the competition for survival in an environment biased by individual variance. In this situation, the theory describes a process where beneficial differences between individuals accumulate in the population over time. The theory describes a natural process where organisms improve their adaptive fit for their environment.

\subsection{Strategy}
The strategy for the Genetic Algorithm is to improve the fitness of a population of candidate solutions in the context of an objective function using repeated rounds of selection and variation. The Genetic Algorithm is inspired by a genetic perspective of the theory of evolution by means of natural selection. The inspiration focuses on a model genetic system of an organism defined by a genome (typically one chromosome and a number of genes). The genome is assessed using an objective function (fitness function) which typically involves translating or mapping the genetic code (genotype) to a problem specific representation (phenotype).

\subsection{Procedure}
The procedure for the Genetic Algorithm involves a large number of iterations called generations where the population is evaluated, the fitter members of which are selected to contribute their genetic material in the creation of the subsequent generation with minor copying errors. The Genetic Algorithm starts with a randomly initialized population of candidate solutions defined as binary strings. Each algorithm iteration, the population is exposed to a series of genetic operator functions that manipulate the population. The candidate solutions are first evaluated against a cost function. A subset of candidate solutions are selected proportional to their relative fitness and recombined to produce new candidate solutions that displace the current population. The creation of new child candidate solutions are random mixes the binary strings of two parents. Low frequency random copying errors are introduced during the recombination process providing a background level of diversity.

\begin{lstlisting}
Initialization
Loop 
	Evaluation
	Selection
	Recombination
	Mutation
	Replacement
End
\end{lstlisting}

\subsection{Heuristics}
The heuristics for the Genetic Algorithm are moderate population sizes, not too greedy, not too random, and try to preserve patterns of genetic material across generations.

\begin{itemize}
	\item Binary strings are the standard genetic representation when implementing the algorithm and its genetic operators, although domain specific representations may be used and the operators modified to support domain specific constraints
	\item The selection process must be balanced between random selection and greedy selection to biased towards fitter candidate solutions (exploitation), whilst promoting useful diversity into the population (exploration). 
	\item Recombination is intended to create new candidate solutions with the favorable attributes of both parents, and as such should attempt to preserve patterns (sub-sequences and/or patterns) of both parents.
	\item Mutation is a low frequency event determined probabilistically for each position in the genome (locus) where the optimal probability is proposed to be 1.0/L where L is the length of the binary string.
	\item Population sizes must be large enough to provide a sufficiently diverse pool of building blocks for the process to be effective, typically equal to the number bits in a candidate solution or some function thereof.
\end{itemize}

\subsection{Tutorial}

\subsubsection{Introduction}
This tutorial covers how to implement a genetic algorithm that operates on candidate solutions with binary genomes to solve the OneMax problem.

\subsubsection{Problem}
The problem instance is called OneMax which scores binary strings based on the number of 1's in the string. It is a popular objective function for demonstrating the Genetic Algorithm as it does not required the binary string to be mapped to another domain, allowing the direct evaluation. 

\begin{lstlisting}
def count_ones(bitstring)
  bitstring.inject(0) {|sum, x| sum + ((x=='1') ? 1 : 0)}
end
\end{lstlisting}

The problem specific mapping and evaluation of solutions by an oracle objective function is computationally the most time consuming aspect of the genetic algorithm.

The OneMax function demonstrates an important principle for the genetic algorithm, and that is relationship of an oracle objective function used for the atomic assessment of candidate solutions. The objective function (or fitness function) is a black box to the algorithm, providing numerical scorings of candidate solutions that only have meaning in the context of other assessed candidate solutions to which the algorithm has been exposed.

In the case of the OneMax problem, the algorithm will receive the same scoring for two very different strings (for example 111000 and 100101). This highlights the generality of the algorithm, where the algorithm solves through a process of induction using only fitness information. For example, if 111000 becomes 111001, the algorithm conserves the new string with the explicit knowledge that the improvement in fitness was caused by making a `0' become a `1'. 

\subsubsection{Solution}
The solution is represented as a binary string genome, in this case an array of characters where each position in the array can have the value of `0' or `1'. Each BinarySolution has a fitness value associated with it, evaluated and assigned by the objective function of the OneMaxProblem.

New solutions are seeded with random bit strings and are used at the beginning of an algorithm run. 

\begin{lstlisting}
def initialize_random(length)
  @genome = Array.new(length)
  @genome.fill {|index| Random.next_bool ? "1" : "0"}
end
\end{lstlisting}

The typical case during the run involves the creation of new candidate solutions (offspring) from candidate solutions already selected and evaluated (parents). This recombination process involves copying sequences of bits from each parent, where the copying process (or transcription) of each value occurs with some probability of error.

\begin{lstlisting}
def initialize_recombination(parent1, parent2)
  # ensure first and last positions are never selected (always cross)
  pos = Random.next_int(parent1.genome.length - 2) + 1
  # create a new genome
  @genome = Array.new(parent1.genome.length)
  # recombine the genomes with copy errors
  @genome.fill do |index|
    if index < pos
      @genome[index] = transcribe parent1.genome[index]
    else
      @genome[index] = transcribe parent2.genome[index]
    end
  end
end

def transcribe(value)
  if Random.next_float < heuristic_mutation_rate(@genome.length)
    return ((value == "1") ? "0" : "1" )
  end
  return value
end
\end{lstlisting}

Importantly, the solutions are specific to the problem, in this case binary strings with floating point fitness scorings. The genetic algorithm logic relevant to manipulating solutions (the crossover and mutation procedures) are encapsulated within the solution class and specialized for the character-based binary strings.

\subsubsection{Algorithm}
The algorithm is implemented as a generic (not problem specific) and reusable system that manages a population of candidate solutions over a set of generations in the context of a provided problem definition. The problem provides interfaces for the algorithm to acquire new and recombined candidate solutions, solution evaluation and solution comparisons - encapsulating notions of relative improvement (maximization or minimization). 

As such, the algorithm initializes some internal state at the beginning of each evolve request and executes generations, each of which creates a new population for the existing old population of candidate solutions. The algorithm keeps track of the best solution found to date (as defined by the problem) and stops executing generations after a stop condition is triggered. In this case a set number of generations or an optimal solution is located as defined by the problem (in this case the optimal solution is equal to the number of bits in the problem).

\begin{lstlisting}
def evolve(problem)
  # store problem
  @problem = problem
  # prepare the population and state
  initialize_population
  # evolve until stop condition is triggered
  while not should_stop? do
    @population = evolve_population population
  end
end

def evolve_population(population)
  # evaluate
  population.each do |solution| 
    @problem.cost solution 
    @best_solution = @problem.choose_better @best_solution, solution
  end
  # select
  selected = population.collect {|solution| tournament_select solution, population}
  # recombine and mutate
  new_population = Array.new(population.length)
  population.each_with_index do |solution, index|
    # probabilistic crossover or promotion
    if Random.next_float < heuristic_crossover_rate        
      if index.modulo(2)==0
        new_population[index] = @problem.new_solution_recombine(solution, population[index+1])
      else
        new_population[index] = @problem.new_solution_recombine(solution, population[index-1])
      end
    else
      new_population[index] = solution
    end
  end
  # one more generation has completed
  @generation += 1
  puts "#{@generation}, best: #{@best_solution}"    
  return new_population
end
\end{lstlisting}

The evolve\_population function does all the hard work, first evaluating the population, selecting the breeding set using tournament-based fitness proportionate-selection, and creating the new population from the selected set. Recombination occurs probabilistically with a high probability. When recombination is not used, the candidate solution is directly promoted to the new population.

The fitness tournament is a simple and efficient implementation of selection that involves a set number of bouts (fights) for each position in the breeding set. The bouts a decided based on the candidate solutions assigned fitness, the relative comparisons of which are assessed using the problem definition.

\begin{lstlisting}
def tournament_select(base, population)
  bouts = 1
  while bouts <= heuristic_selection_num_bouts do
    pos = Random.next_int(population.length)
    candidate = population[pos]
    base = @problem.choose_better(base, candidate)
    bouts += 1
  end
  return base
end
\end{lstlisting}

\subsubsection{Summary}
The Genetic Algorithm has a number of parameters which influence the greediness and randomness of the algorithm. Given the number of probabilistic decisions made during each iteration of the algorithm, the same inputs to the algorithm can provide an array of varied output results, such as the best solution found. As such, it is important that the system is executed a number of times for any given configuration. This will provide a more meaningful understanding of the algorithms capability for a configuration on a problem.


\subsection{Further Reading}
The Genetic Algorithm is a general optimization algorithm that models problem domains as binary strings. The lack of problem specific information incorporated into the technique means that it can be rapidly applied against a given problem. This same benefit also means that the technique is fragile to the the mapping of binary strings to the domain and the amount of problem specific information encoded into the fitness evaluation. 

\begin{itemize}
	\item Goldberg
	\item EC1 and EC2
\end{itemize}