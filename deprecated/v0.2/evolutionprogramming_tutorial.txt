Evolutionary Programming Tutorial
Copyright (C) 2008 Jason Brownlee

Change History
2008/12/05  JB  Created

Introduction
This tutorial provides a guide for implementing the Evolutionary Programming algorithm applied to a the non-linear programming problem with continuous parameters called Rastigen's Function.

Problem
The problem is a non-linear programming problem that defines a function with a set of unknown parameters. Such problems are common for testing optimization problems because the number of problems can be varied, to vary the difficulty of the problem. 

def calculate(v)
  (10.0 * @dimensions.to_f) + v.inject(0) {|sum, x| (x**2.0) - 10.0 * Math.cos(2.0 * Math::PI * x) }
end

The problem space is a hypercube with the boundaries of -5.12 and +5.12. The solution to the problem is known where each objective parameter has the value '0'.

Solution
A solution to the problem is represented by an array of floating point numbers (vector). In Associated with the solution and its fitness scoring is an array of strategy parameters, one for each objective parameter. These strategy parameters control the variance genetic operator applied to each objective parameter. 

The population is seeded with candidate solutions seeded with random objective values from within the search space. During the execution of the search, each candidate solution creates a progeny candidate solution from the parents objective and strategy vectors. The variation operator is introduced per-objective parameter and implemented in the transcription of the vectors.

def initialize_offspring(parent, length, min, max)
  # populate strategy parameters
  g = Random::next_gaussian
  @strategy_params = Array.new(length) do |i|
    transcribe_strategy(parent.strategy_params[i], g, length)
  end
  # populate objective values
  @objective_params = Array.new(length) do |i|
    transcribe_objective(parent.objective_params[i], @strategy_params[i], min, max)
  end
end

The mutation of the strategy occurs based on some heuristics proportional to the number of parameters in the problem.

def transcribe_strategy(x, g, dimensions)
  x * Math::exp((heuristic_rprime(dimensions) * g) + (heuristic_r(dimensions) * Random::next_gaussian))
end

Give that a solution is represented by floating point numbers, a Gaussian-based mutation operator is applied. This operator involves the re-sampling an objective parameter using a Normal distribution where the current value represents the mean, and the standard deviation is determined by the corresponding strategy parameter. New objective values are bounds tested to ensure they are valid in the context of the problem (within the constraints of the problems parameter space).

def transcribe_objective(x, stdev, min, max)
  o = x + stdev * Random::next_gaussian
  o = min if o < min
  o = max if o > max
  return o
end

Algorithm
The Evolutionary Programming algorithm is implemented as a reusable system for executing the strategy on a problem and producing an approximation of the optimal solution. The strategy is executed by calling the evolve function that initializes the population and executes each generation until the stop condition is met. In this case, the stop condition is defined as a set number of generations or whether the known optimal solution has been discovered.

def evolve(problem)
  # store problem
  @problem = problem
  # prepare the initial population
  initialize_population
  # evaluate the initial population
  evaluate_population(population)
  # evolve until stop condition is triggered
  while !should_stop? do
    # create the new population
    @population = evolve_population(@population)
  end
end

A single generation involves the creation of a new population of offspring (one per parent), and selecting the candidate solutions from the parent and child populations to comprise the next generation. 

def evolve_population(population)
  # recombine and mutate
  offspring = Array.new(population.length) do |i|
    @problem.new_solution_offspring(population[i])
  end
  # evaluate the newly created candidate solutions
  evaluate_population(offspring)
  # combine the existing and new populations
  union = population + offspring
  # let the solutions compete for survival    
  competitive_tournaments(union)
  # shuffle the union in case few solutions win
  Random.shuffle_array(union)
  # order the union by wins desc
  union.sort! { |a,b| b.wins <=> a.wins }
  # select the winners for the new population
  winners = union[0...population.length]
  # one more generation has completed
  @generation += 1
  puts "generation:#{@generation}, #{@best_solution}"    
  return winners
end

Competition involves combining the parent and offspring populations and for each member of this new set, comparing its fitness to a set number of opponents. If the candidate solutions is better than all opponents which it faces then it is awarded a point. After all bouts are completed the next generation of candidate solutions is selected based on the number of wins each solution was awarded during the competitive process.

def competitive_tournaments(pop)
  # clear wins
  pop.each {|s| s.wins = 0 }
  # to get a point, each solution must better than a set of random peers
  pop.each do |s|
    better = true
    heuristic_num_opponents.times do |i|
      pos = Random::next_int(pop.length)
      better = false if @problem.is_better?(s, pop[pos])
    end
    s.wins += 1 if better
  end
end

Summary
The Evolutionary Programming algorithm is very similar to ES, and in this case was demonstrated without a recombination operator and with a 'soft' selection mechanism designed to ensure the search is not too greedy. Natural extensions involves alternative heuristics for adapting the strategy parameters, the use of a recombination operator and varied selection mechanisms.