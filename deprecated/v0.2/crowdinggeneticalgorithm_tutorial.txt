Crowding Genetic Algorithm Tutorial
Copyright (C) 2008 Jason Brownlee

Change History
2008/12/12  JB  Created

Overview
This tutorial provides an example of the deterministic crowding genetic algorithm, the seminal approach for crowding based genetic algorithms. The algorithm is applied to a multi-modal optimization problem called M1 from "Niching Methods for Genetic Algorithms" (1995).

Problem
The problem is a non-linear programming problem with 5 optimal solutions [0.1, 0.3, 0.5, 0.7, 0.9] all with the fitness of 1.0. As such this problem (M1) is a one-dimensional maximizing multi-modal, with five peaks.

def calculate(x)
  Math.sin(5.0 * Math::PI * x) ** 6.0
end

Solution
The solution is modelled using a bitstring genome which is decoded into a floating point value in the range [0,1]. New solutions are initialized with random strings.

def initialize_random(length)    
  @genome = Array.new(length) {|i| Random.next_bool ? "1" : "0"}
end

During a run new solutions are created by recombining two parents. One point crossover is used with mutation implemented during the coping process.

def initialize_recombination(parent1, parent2)
  length = parent1.genome.length
  # select a cut position
  cut = Random.next_int(length - 2) + 1
  # recombine the genomes with copy errors
  @genome = Array.new(length) do |i| 
    (i<cut) ? transcribe(parent1.genome[i], length) : transcribe(parent2.genome[i], length) 
  end
end

def transcribe(value, length)
  if Random.next_float < heuristic_mutation_rate(length)
    return ((value == "1") ? "0" : "1" )
  end
  return value
end

The phenotype is calculated on demand (lazy evaluation) and involves first decoding the value. A generic decode function is used, taken from (EC 1, chapter ?, page ?, equation ?).

def phenotype
  @decoded_value = BinarySolution.decode(@genome,@min,@max) if @decoded_value.nan?
  return @decoded_value
end  

# generic decode function for bitstring to float in [min,max]
def self.decode(bitstring, min, max)
  sum = 0
  bitstring.each_with_index do |x, i|
    sum += ((x=='1') ? 1 : 0) * (2 ** i)
  end
  # rescale [0,2**L-1] to [min,max]
  return min + ((max-min) / ((bitstring.length**2.0) - 1.0)) * sum
end

A phenotypic distance measure is used, in this case the absolute distance between the two one-dimensional phenotypes.

def dist(other)
  return (phenotype - other.phenotype).abs    
end

Algorithm
The algorithm involves initializing the base population, evaluating the randomly created solutions, and executing generations until the stop condition is triggered.

def evolve problem
  # store problem
  @problem = problem
  @population = Array.new(heuristic_population_size) {|i| @problem.new_solution}    
  # evaluate the base population
  @population.each {|s| @problem.cost(s)}
  # evolve until stop condition is triggered
  @generation = 0
  next_generation(@population) until stop_triggered?
end

The stop condition is defined as finiding all 5 optima, or completing a fixed number of generations.

def stop_triggered?
  (@generation==heuristic_total_generations) or @problem.found_all_optima?(@population)
end

Each generation the entire population participating in recombination to create offspring. The population is first randomly shuffled to ensure the pairings are random and different each generation. For a given pair, two offspring are created via recombination. The offspring are evaluated involving their being decoded to a problem-specific phenotype, and an objective fitness scoring assigned. Each parent is assigned the most similar offspring to compete with based on phenotypic similarity. The competition is for the position in the population and the winner is selected based on the better objective fitness value.

def next_generation(pop)
  # shuffle the population
  # Random.shuffle_array(pop)
  # the entire population participates in reproduction
  (pop.length/2).times do |i|
    # select parents [[0,1],[2,3],etc]
    a = (i*2)
    b = (i*2)+1
    p1 = pop[a]
    p2 = pop[b]
    # create offspring
    o1 = @problem.new_solution_recombine(p1, p2)
    o2 = @problem.new_solution_recombine(p2, p1)
    # evaluate
    @problem.cost(o1)
    @problem.cost(o2)
    # compete for positions in the population based on similarity then fitness
    if (p1.dist(o1) + p2.dist(o2)) <= (p1.dist(o2) + p2.dist(o1))        
      pop[a] = @problem.choose_better(p1, o1)
      pop[b] = @problem.choose_better(p2, o2)
    else
      pop[a] = @problem.choose_better(p1, o2)
      pop[b] = @problem.choose_better(p2, o1)
    end
  end
  # one more generation has completed
  @generation += 1
  puts "#{@generation}, avg(#{average_fitness}), found #{@problem.num_optima_found(@population)}/#{@problem.num_optima}"
end


Extensions
- Easy: Try applying the approach to other multimodal problems with higher dimensionality (foxholes, camel back, etc, swachfules)
- Medium: Implementing a generic function for clustering the final population and presenting the results. 
- Advanced: More advanced extensions involve implementing other crowding techniques, such as RTS, PC, and such.
