% The Clever Algorithms Project: Algorithm Selection

% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2010 Jason Brownlee. All Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

\documentclass[a4paper, 11pt]{article}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{url}
\usepackage[pdftex,breaklinks=true,colorlinks=true,urlcolor=blue,linkcolor=blue,citecolor=blue,]{hyperref}
\usepackage{geometry}
\geometry{verbose,a4paper,tmargin=25mm,bmargin=25mm,lmargin=25mm,rmargin=25mm}

% Dear template user: fill these in
\newcommand{\myreporttitle}{The Clever Algorithms Project}
\newcommand{\myreportsubtitle}{Algorithm Selection}
\newcommand{\myreportauthor}{Jason Brownlee}
\newcommand{\myreportemail}{jasonb@CleverAlgorithms.com}
\newcommand{\myreportproject}{The Clever Algorithms Project\\\url{http://www.CleverAlgorithms.com}}
\newcommand{\myreportdate}{20100112}
\newcommand{\myreportversion}{1}
\newcommand{\myreportlicense}{\copyright\ Copyright 2010 Jason Brownlee. All Rights Reserved. This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.}

% leave this alone, it's templated baby!
\title{{\myreporttitle}: {\myreportsubtitle}\footnote{\myreportlicense}}
\author{\myreportauthor\\{\myreportemail}\\\small\myreportproject}
\date{\today\\{\small{Technical Report: CA-TR-{\myreportdate}-\myreportversion}}}
\begin{document}
\maketitle

% write a summary sentence for each major section
\section*{Abstract} 
This is the abstract. Consider writing a one sentence summary of each major section in the report.

\begin{description}
	\item[Keywords:] {\small\texttt{Clever, Algorithms, Algorithm, Selection, Methodology}}
\end{description} 

% summarise the document breakdown with cross references
\section{Introduction}
\label{sec:introduction}
% project
The Clever Algorithms project aims to describe a large number of algorithms from the field of Artificial Intelligence in a complete, consistent, and centralized way to improve the accessibility of the methods \cite{Brownlee2010}. 
% report
This report focuses on the methodology for selecting algorithms that appear in the project and provides a preliminary suggested listing of algorithms to describe. 
% breakdown
Section~\ref{sec:methodology} describes a data-driven methodology for organizing, evaluating, and selecting algorithms. Section~\ref{sec:results} summarizes the raw results of applying the methodology in terms of the ranked algorithms. Section~\ref{sec:analysis} analyzes the presented results and comments on areas for improvement in the methodology. Section~\ref{sec:selection} provides a preliminary listing of algorithms selected to be described in the Clever Algorithms project. The listing is not final, rather it is presented as a first draft to be refined over the lifetime of the project. Section~\ref{sec:conclusions} reviews the findings of the process, and highlights some areas for future consideration.

% 
% Methodology
% 
\section{Methodology}
\label{sec:methodology}
This section describes the methodology for the selection of algorithms for inclusion in the Clever Algorithms project. This methodology and its results are based on a post on the blog `Never Read Passively' by Jason Brownlee (this author) on August 2nd 2009 entitled ``What is a good optimization algorithm? A data-driven method for algorithm selection''\footnote{Online: \url{http://www.neverreadpassively.com/2009/08/what-is-good-optimization-algorithm.html}}. The method presented in this previous work was based on the question: \emph{Is it possible to select a diverse, interesting, and useful set of inspired algorithms using a simple data driven method?}. The methodology presented in this section is updated to include more sources used for algorithm comparison and use a weighted sum when calculating an algorithms ranking. The application of this methodology was updated to use an automated script, and the listing of algorithms for evaluation was updated and refined. The methodology is presented in terms of the three core tasks in the process: algorithm list, algorithm ranking, and algorithm selection.

% 
% Algorithm List
% 
\subsection{Algorithm List}
The first task involve the preparation of a large listing of candidate algorithms. The algorithms may be drawn from the an array of fields under or related to the fields of Biologically Inspired Computation and Computational Intelligence, such as: Metaheuristics, Hyperheuristics, Natural Computation, Biomementics, Swarm Intelligence, Collective Intelligence, and Evolutionary Computation.

The sources for algorithms names should be diverse and may include books, articles, papers, magazines, websites, and software. The specification of the algorithm in the list is important. Algorithm name standardization is an important aspect of the methodology. The algorithm should be listed in their canonical (most common) name, normalized, without acronyms (wherever reasonably possible), and in their full unabbreviated form

Each algorithm should be assigned a kingdom for taxonomic reasons. The kingdom is used to group algorithms which defines the scope of competition that the algorithms must survive to be included in the project. Kingdoms may be superficial names (such as the taxonomic names the algorithms may be assigned in book chapters general observation and intuition), such as: evolutionary, immunological, swarm, physical, probabilistic, and stochastic.

% 
% Algorithm Rankings
% 
\subsection{Algorithm Rankings}
Each algorithm in the prepared listing must be evaluated, assigned a scoring, and ranked to allow selection decisions to be made. A data-driven popularity-based approach is proposed for algorithm evaluation. Each algorithm name is submitted to an array of different domain specific search engines and the total approximated number of results reported by the search services are taken as measures. The submission of the algorithm name is standardized such that it is surrounded by quotes to ensure that the search is specific and the algorithm name is converted to lower case. 

Some example search domains to which algorithm names may be submitted include:
\begin{itemize}
	\item \emph{Google Web Search}: Search of webpage on the internet index by Google.
	\item \emph{Google Book Search}: Search of books index by Google.
	\item \emph{Google Scholar Search}: Search of the academic publications and patents index by Google.
	\item \emph{Springer Article Search}: Search of the articles and papers published by Springer.
	\item \emph{Scirus Article Search}: Search of the articles and papers indexed by Scirus.
	\item \emph{IEEE Article Search}: Search of the articles and papers published by IEEE.
	\item \emph{JSTOR Article Search}: Search of the articles and papers scanned and indexed by JSTOR.
	\item \emph{CiteSeer Article Search}: Search of the articles and papers indexed by CiteSeer.
	\item \emph{ACM Article Search}: Search of the articles and papers indexed by ACM Digital Library.
	\item \emph{Amazon Book Search}: Search of the books indexed and sold by Amazon.
\end{itemize}

The collected measures are representative of how often an algorithm name appears in a particular search domain, and inferred to represent the popularity to which a given algorithm is written about and studied. This data-driven algorithm evaluation strategy may be simply stated as \emph{An algorithm or method is as good its name, as useful and interesting as it is abundant}, or more specifically, an algorithm that is used (written about) is an algorithm that is useful (worth knowing about). 

The collected measures are then combined to produce a scoring for a given algorithm name. The combination may be a weighted sum of the measures, where different weighting factors may be used to increase the influence of a particular search domain. For example an algorithms's occurrence in published books may have more importance than an algorithms occurrence on webpages. 

Finally, the calculated algorithms scores are used to compare algorithms. Algorithm comparisons may be direct, or may be scoped, such as by using an algorithms allocated taxonomic kingdom.

% 
% Algorithm Selection
% 
\subsection{Algorithm Selection}
The selection of the algorithms that are considered in the project is as important, if not more important, than the descriptions of the algorithms themselves. The particular mixture of algorithms must be include popular techniques that readers are expected to lookup, although the pool must be diverse enough to be interesting and promote discovery. As such, the selection of algorithms may not rely upon the ranking of algorithms alone. Algorithms may be sorted by ranking and kingdom which provide an indication of algorithm popularity. The sorted ranked algorithms may then filtered using using indicators for algorithm interestingness and diversity. Such measures are difficult to quantify and automate and so may rely on the subjective human assessments of an expert. The popularity-based ordering imposed by the list sorted by ranking may not be appropriate for presentation in the project. 

% 
% Results
% 
\section{Results}
\label{sec:results}
This section summarizes the results of assessing algorithms for inclusion in the Clever Algorithms project.

% 
% Configuration
% 
\subsection{Configuration}
This section describes the configuration under which the data was collected, as follows: 

\begin{itemize}
	\item The algorithm list was prepared with algorithms collected primarily for optimization rather than model building. These algorithms are referred to as `unconventional optimization algorithms', and subfields that were intentionally neglected include artificial neural networks, fuzzy logic, and statistical machine learning subjects such as kernels.
	\item The algorithms were divided into 6 taxonomic kingdoms: evolutionary, immunological, swarm, physical, probabilistic, and stochastic, with a 7th kingdom used to rank names of related fields (for general interest).
	\item The search engines included as algorithm measures included: Google Web, Google Book, Google Scholar, Springer Article, and Scirus Article.
	\item The data collection process was automated with a ruby script that performs the searches, collated the results, and generated the final statistics and ordered listings.
	\item The measures from the search domains were weighted when combined into the algorithm scoring as follows: todo
\end{itemize}

% 
% Raw Data
% 
\subsection{Raw Data}
This section described the raw data collected as a result of following the experimental methodology in terms of the algorithm kingdom, standardized algorithm name, specific measures from different search domains, and the allocated ranking. The results were collected from the specified search engines on January 12th 2010.

\subsubsection{Algorithm Fields}
todo

\subsubsection{Evolutionary Algorithms}
todo

\subsubsection{Stochastic Algorithms}
todo

\subsubsection{Probabilistic Algorithms}
todo

\subsubsection{Swarm Algorithms}
todo

\subsubsection{Immune Algorithms}
todo

\subsubsection{Physical Algorithms}
todo

\subsubsection{Overall Top 10 Algorithms}
todo

% 
% Analysis
% 
\section{Analysis}
\label{sec:analysis}
This section analyses the results of the assessed algorithms for inclusion in the Clever Algorithms project. The analysis focuses on the limitations of the adopted algorithm selection strategy, and the identification of potential areas for improvement.

% Names
\subsection{Algorithm Names}
Not all algorithm names are created equal, and as such, using algorithm names alone to compare the popularity of algorithmic techniques provides an opportunity for improvement. For some algorithm names, such as the `genetic algorithm', the name represents a base name (and typically a base technique) for a family techniques. This results in artificially inflated popularity scorings. Additionally, some techniques are renamed one or more times throughout their life, and some techniques are better known by abbreviations or acronyms then their full unabbreviated names.

Names are commonly used to delineate areas of research, not necessarily specific algorithmic techniques intended for application. For example, there is no `canonical genetic algorithm', there is instead a general procedure and a number of variant operators that may be applied at each step. Although, in this case, there does happen to be a `simple genetic algorithm' that could be considered canonical, and separately, a `canonical genetic algorithm' may be deduced from the best practice realization of each operator in the procedure. This is both an exemplar that highlights both the confusion in the field and an opportunity that that Clever Algorithms project may help address.

Finally, in the same manner that the popularity of algorithms are tested by assessing their names, the popular association of an algorithm and a field or taxonomic kingdom may be determined using a data-driven method. A set of field names may be defined and the popularity of each algorithm name with each field name may be calculated, scored, and ranked. 

% Scoring
\subsection{Algorithm Scoring}
The number of results returned from each search engine is an approximation with an unknown level of error, both with regard to the service reporting hits from its known index, and to the extent that index is not complete. Another area of concern with the collected measures is that it does not take into account when algorithms were released. For examples, those techniques released many years ago will likely be more popular than those released very recently. This is likely a desirable effect of the method, although may mask new algorithms whose popularity is rising and old algorithms whose popularity is falling. 

Finally, the measures are absolute quantities that are not likely to be comparable between search domains. This was tentatively addressed though the introduction of weighting factors in the calculation of the algorithm scoring. An alternative method may involve normalizing the results for each domain before combining them into a single score.

Ideally, popularity would not be the basis for algorithm comparison. Some more useful measures may include efficiency, efficacy, quality of research, and scope of innovation. These are all properties that are difficult to quantify and difficulty to measures, especially in using objective and automated means.  

% Selection
\subsection{Algorithm Selection}
Algorithm popularity may be useful for determining the ordering by which algorithms are described in the project (for marketing and discoverability purposes for example), although may not be appropriate for the ordering by which algorithms are listed in deliverable outcomes of the project, such as a book or website. A more appropriate ordering for these cases may be a logical progression through algorithm complexity, determined by difficulty (time) required to understand and/or implement a given algorithm.  

% 
% Selected Algorithms
% 
\section{Selected Algorithms}
\label{sec:selection}
This section provides a listing of 50 algorithms selected for description in the Clever Algorithms Project. The presentation of this list of algorithms is partitioned into taxonomic kingdoms. Each algorithm is listed with at least one primary or seminal source to verify the existence of the approach. This listing of algorithms is not final, rather, it is a preliminary draft based on the best information available at the time of writing. It is expected that the specific algorithms described in the Clever Algorithms project will differ as the project unfolds. This preliminary listing is suggested as a starting point for breakdown for the project deliverable outcomes, such as book chapters and sections.

\subsection{Stochastic Algorithms}
todo

\subsection{Evolutionary Algorithms}
todo

\subsection{Swarm Algorithms}
todo

\subsection{Immune Algorithms}
todo

\subsection{Probabilistic Algorithms}
todo

\subsection{Physical Algorithms}
todo

% 
% Conclusions
% 
\section{Conclusions}
\label{sec:conclusions}
This report proposed a data-driven methodology for selecting algorithms based on popularity. This methodology was applied for the proposes of selecting algorithms to describes in the Clever Algorithms project. The raw results were presented for posterity, and a preliminary set of 50 algorithms were selected for the project. This listing of algorithms is not expected to be maintained within this document, although is expected to represent a first draft for breakdown for the deliverable outcomes of the Clever Algorithms project, such as a book and website. 

The listing focused on so-called `unconventional optimization algorithms', intentionally neglecting model-based algorithms such as artificial neural networks and fuzzy logic systems. If the Clever Algorithms project proves successful, the effort may consider the preparation of a second volume (book) that shifts the focus of the project from optimization algorithms to model-creating algorithms and address neural network, fuzzy logic, and some statistical machine learning algorithms that fit within the scope of the project.

% bibliography
\bibliographystyle{plain}
\bibliography{../bibtex}

\end{document}
% EOF